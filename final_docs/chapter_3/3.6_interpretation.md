# 3.6 การแปลผลและอธิบายแบบจำลอง (Model Interpretation)

หนึ่งในความท้าทายสำคัญของการใช้แบบจำลองการเรียนรู้ของเครื่องขั้นสูง (Advanced Machine Learning) และการเรียนรู้เชิงลึก (Deep Learning) คือลักษณะความเป็น "กล่องดำ" (Black-box) ซึ่งยากต่อการอธิบายกระบวนการตัดสินใจ เพื่อให้ผลการวิจัยมีความโปร่งใสและสามารถนำไปใช้อ้างอิงทางวิทยาศาสตร์ได้ งานวิจัยนี้จึงประยุกต์ใช้เทคนิค **Explainable AI (XAI)** เพื่อวิเคราะห์อิทธิพลของตัวแปรต่างๆ ที่มีต่ออุณหภูมิพื้นผิวดิน

### 3.6.1 SHAP (SHapley Additive exPlanations)

**SHAP** เป็นวิธีการที่พัฒนาขึ้นจากทฤษฎีเกม (Game Theory) เพื่อวัดค่าการมีส่วนร่วม (Contribution) ของแต่ละฟีเจอร์ที่มีต่อผลการทำนาย ในการศึกษานี้ ผู้วิจัยใช้ SHAP ใน 2 ระดับ:

1.  **Global Interpretation:** การหาค่าเฉลี่ยของค่า SHAP (Mean Absolute SHAP Value) จากข้อมูลทั้งหมด เพื่อจัดลำดับความสำคัญของตัวแปรในภาพรวมว่า ก๊าซเรือนกระจกชนิดใด ($CH_4, NO_2, CO$) หรือปัจจัยสภาพแวดล้อมใดส่งผลต่อ LST มากที่สุด
2.  **Local Interpretation:** การวิเคราะห์ค่า SHAP ในระดับรายพิกเซลหรือรายพื้นที่ (Spatial Analysis) เพื่อทำความเข้าใจว่าในพื้นที่เฉพาะ (เช่น เขตเมือง หรือเขตเกษตรกรรม) ปัจจัยใดเป็นตัวขับเคลื่อนหลักที่ทำให้อุณหภูมิสูงขึ้นหรือลดลง ซึ่งจะช่วยให้เห็นความสัมพันธ์เชิงพื้นที่ที่ซับซ้อนได้ชัดเจนยิ่งขึ้น

### 3.6.2 Permutation Feature Importance

เพื่อยืนยันผลลัพธ์ที่ได้จาก SHAP ผู้วิจัยได้ใช้วิธี **Permutation Feature Importance** ร่วมด้วย หลักการคือการวัดประสิทธิภาพของโมเดลที่ลดลงเมื่อทำการ "สลับค่า" (Shuffle) ของฟีเจอร์หนึ่งๆ แบบสุ่ม หากการสลับค่าของฟีเจอร์ใดทำให้ค่าความแม่นยำ ($R^2$) ลดลงอย่างมาก แสดงว่าฟีเจอร์นั้นมีความสำคัญสูงต่อการทำนาย

การใช้ทั้งสองวิธีควบคู่กัน (SHAP และ Permutation Importance) จะช่วยยืนยันความถูกต้องของผลการวิเคราะห์ปัจจัยปัจจัยขับเคลื่อน (Drivers) ของปรากฏการณ์เกาะความร้อนและการเปลี่ยนแปลงอุณหภูมิในประเทศไทยได้อย่างน่าเชื่อถือ
